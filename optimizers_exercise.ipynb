{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqpWwhDLyHZ-"
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "## 1 Linear model gradients\n",
    "This week we discussed gradient descent in its various forms. We talked about some of the pitfalls that can be encountered and how we can overcome them. In this notebook, we go through a variety of mechanisms that improve on the vanilla implementation of gradient descent. To start with, let's create a small, random dataset that will help us to visualise how different methods for optimizing work over a simple loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3gHJN26cyHaC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# a change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3dq1IoUyHaD"
   },
   "source": [
    "We will use a dataset generated from a line with noised added:\n",
    "$$ y = x + 1 + \\epsilon $$\n",
    "$$ x \\in [0,1]. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "9RvjYOvZyHaE",
    "outputId": "656b8580-bf99-4245-c23e-59568e996614"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.05 0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65\n",
      " 0.7  0.75 0.8  0.85 0.9  0.95 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Create constantly-spaced x-values\n",
    "x = np.linspace(0,1,21)\n",
    "\n",
    "# Create a linear function of $x$ with slope 1, intercept 1, and normally distributed error with sd=1\n",
    "y = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "plt.plot(x,y,'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fIBGswUyHaG"
   },
   "source": [
    "Our cost function (which we could also refer to as the negative log-likelihood) is simple least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAVUoXPayHaG"
   },
   "outputs": [],
   "source": [
    "# Define cost function (sum square error between prediction and data points)\n",
    "def L(w,x,y):\n",
    "    return 1./2.*sum((y - w[0] - w[1]*x)**2)\n",
    "\n",
    "# Evaluate the cost function at many values of slope and intercept, aka the brute force method.  \n",
    "# To be used for visualization purposes\n",
    "L_grid = np.zeros((101,101))\n",
    "w0s = np.linspace(0,1.5,101)\n",
    "w1s = np.linspace(0,1,101)\n",
    "for i,w1 in enumerate(w1s):\n",
    "    for j,w0 in enumerate(w0s):\n",
    "        L_grid[i,j] = L([w0,w1],x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCmo0_zhyHaH"
   },
   "source": [
    "Recall that to solve the least squares problem, we take the derivative of this thing with respect to the weights, set them equal to zero.  This produces the normal equations:\n",
    "$$\n",
    "\\Phi^T \\Phi W = \\Phi^T y\n",
    "$$\n",
    "which have an analytical solution.  However, for the purposes of illustration, we can assume that we can't just solve them, and have to use gradient descent.  The gradient for the intercept and the slope of the line we want to fit are \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_0} = -\\sum_{i=1}^m (y_i - w_0 - w_1 x_i)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = -\\sum_{i=1}^m (y_i - w_0 - w_1 x_i) x_i\n",
    "$$\n",
    "Writing a python function for this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDy8bWHhyHaI"
   },
   "outputs": [],
   "source": [
    "# Gradient function\n",
    "def G(w,x,y):\n",
    "    return np.array([-sum(y - w[0] - w[1]*x),-sum((y - w[0] - w[1]*x)*x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAKidNLbyHaI"
   },
   "source": [
    "## 2 Batch Gradient Descent\n",
    "### Exercise: Implement vanilla gradient descent\n",
    "\n",
    "First, we can run so-called batch gradient descent, in which we compute the objective function considering **all the data points at once**. This is referred to as a complete batch.  We'll save our weights at each epoch, so that we can plot them.  A learning rate $\\eta=0.01$ works well for this problem. Your first task is to implement vanilla gradient descent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxXg5OmXyHaJ"
   },
   "outputs": [],
   "source": [
    "# Create an initial guess for the weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize a list to hold our weight values at each step of gradient descent\n",
    "w_batch = [w.copy()]\n",
    "\n",
    "# Set the learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Loop over the data 10000 times\n",
    "for i in range(10000):\n",
    "\n",
    "    gradient = G(w,x,y) \n",
    "    # Update the weights (w) by taking a small step in the negative direction of the gradient\n",
    "    \n",
    "    # TODO: Implement this update using the learning rate (eta) and the gradient\n",
    "    w = None\n",
    "    \n",
    "    # Append the new parameters to our list of weights\n",
    "    w_batch.append(w.copy())\n",
    "    \n",
    "# Convert the list to a numpy array\n",
    "w_batch = np.array(w_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhSal9tXyHaJ"
   },
   "source": [
    "Let's visualise the path that gradient descent takes to the minimum with a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-W_mnSb1yHaJ"
   },
   "outputs": [],
   "source": [
    "# Make a contour plot of the grid of cost-function values\n",
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "\n",
    "# Plot the weight values on our way toward the cost-function minimum\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4gVrfO8yHaK"
   },
   "source": [
    "## 3 Stochastic gradient descent\n",
    "It's not always possible to fit all the data in memory, and so we have to do something different.  One simple thing that we can do is to compute the gradient of the objective function considering only *one datapoint at a time*, sampled at random without replacement from the dataset, hence the name stochastic gradient descent. \n",
    "\n",
    "We'll save both the weight values at these individual steps, and also the weights at the end of each *epoch*, which is what we call the outer iteration in which we look at each data point once; in batch gradient descent, we look at all the data at once as a single batch, so an epoch corresponds to one iteration of BGD.  In stochastic gradient descent, we'll take $m$ steps of gradient descent per epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7YI4E5ayHaK"
   },
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize arrays to hold weights\n",
    "w_stoch = [w.copy()]\n",
    "w_epoch = [w.copy()]\n",
    "\n",
    "# Define learning rates\n",
    "eta = 0.01\n",
    "\n",
    "# Train for 10000 epochs\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Draw random indices of the dataset\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    \n",
    "    # Loop over all of the data points without replacement\n",
    "    for j in random_indices:\n",
    "        \n",
    "        # Take as a sample the j-th element in the training data\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        \n",
    "        # Take a gradient descent step based on that single data point\n",
    "        w -= eta*G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        w_stoch.append(w.copy())\n",
    "    \n",
    "    # Store the weights at the end of the epoch\n",
    "    w_epoch.append(w.copy())\n",
    "\n",
    "# Convert lists to arrays\n",
    "w_stoch = np.array(w_stoch)\n",
    "w_epoch = np.array(w_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l4CgA-JyHaL"
   },
   "source": [
    "Let's plot this on top of the results from batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za8ckrPdyHaL"
   },
   "outputs": [],
   "source": [
    "# Plot the error surface\n",
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "\n",
    "# Plot the results of batch gradient descent in green\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "\n",
    "# Plot the results of stochastic gradient descent in blue\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_epoch[:,0],w_epoch[:,1],'bo')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niFPEnl2yHaL"
   },
   "source": [
    "There are some wiggles, but the stochastic value after each epoch falls remarkably close to the batch descent line.  This is even more interesting if we zoom in on the upper right region (near convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8aDNfhTyHaL"
   },
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_epoch[:,0],w_epoch[:,1],'bo')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVruCR8cyHaM"
   },
   "source": [
    "Why does this work?  Let's look at the sum of the individual weight updates in SGD over an epoch.  An update based on a single data point would be:\n",
    "$$\n",
    "\\Delta w_{i} = \\eta [y_i - w_{0,i} - w_{1,i} x_i, (y_i - w_{0,i} - w_{1,i}) x_i].\n",
    "$$\n",
    "What does the update after an entire epoch (i.e. a complete loop through the data) look like.  That would just be the sum over all $m$ updates in the \"inner\" loop, with the constant step size moved outside the sum (summation is linear after all):\n",
    "$$\n",
    "\\Delta w_{SGD} = \\eta [\\sum_{i=1}^m (y_i - w_{0,i} - w_{1,i} x_i),\\sum_{i=1}^m (y_i - w_{0,i} - w_{1,i} x_i)x_i]\n",
    "$$\n",
    "Compare this to the update for batch gradient descent\n",
    "$$\n",
    "\\Delta w_{BGD} = \\eta [\\sum_{i=1}^m (y_i - w_{0} - w_{1} x_i),\\sum_{i=1}^m (y_i - w_{0} - w_{1} x_i)x_i]\n",
    "$$\n",
    "You'll note that it's exactly the same, with the exception of the subscripts on the weights.  However, since the weights aren't changing very rapidly (we're taking small steps after all), the resulting updates are very close to identical: all the little steps taken in SGD average out to produce something quite close to batch gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG0Wj66Mk7LD"
   },
   "source": [
    "## 4 Mini-batch gradient descent for linear regression\n",
    "\n",
    "These are two end-member options for dealing with gradient descent.  The best solution for the purposes of machine learning lies somewhere in the middle, via a technique called mini-batch gradient descent.  In mini-batch gradient descent, at each epoch we split the data-set into $k$ subsets of a specified size known as the *batch size*, and take $k$ steps based on the objective function gradient considering only the $m/k$ data points in a given mini-batch.\n",
    "\n",
    "Notice that this method A) creates random disjoint subsets of the training data of the size $m/k$, and B) updates the weights $k$ times per epoch.  The results of mini-batch gradient descent are plotted below along with the batch and stochastic gradient descents from above, using the same initial guess for $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0B_CzBayHaM"
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "# Initialize weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize arrays to hold weights\n",
    "w_stoch = [w.copy()]\n",
    "w_epoch = [w.copy()]\n",
    "\n",
    "# Define learning rates\n",
    "eta = 0.01\n",
    "\n",
    "# Train for 10000 epochs\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Draw random indices of the dataset\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    slices = []\n",
    "    for i in range(7):\n",
    "        slices.append(random_indices[i:21:7])\n",
    "    \n",
    "    x_sample = np.zeros(3)\n",
    "    y_sample = np.zeros(3)\n",
    "\n",
    "    # Loop over all of the data points without replacement\n",
    "    for _slice in slices:\n",
    "        count = 0\n",
    "        for j in _slice:\n",
    "            # Take as a sample the j-th element in the training data\n",
    "            x_sample[count] = x[j]\n",
    "            y_sample[count] = y[j]\n",
    "            count += 1\n",
    "        \n",
    "        # Take a gradient descent step based on that single data point\n",
    "        w -= eta*G(w,x_sample,y_sample)\n",
    "        w_stoch.append(w.copy())\n",
    "    \n",
    "    # Store the weights at the end of the epoch\n",
    "    w_epoch.append(w.copy())\n",
    "\n",
    "# Convert lists to arrays\n",
    "w_stoch = np.array(w_stoch)\n",
    "w_epoch = np.array(w_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTqf1eO0yHaM"
   },
   "outputs": [],
   "source": [
    "# Plots here\n",
    "# Plot the error surface\n",
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "\n",
    "# Plot the results of batch gradient descent in green\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "\n",
    "# Plot the results of stochastic gradient descent in blue\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_epoch[:,0],w_epoch[:,1],'bo')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_epoch[:,0],w_epoch[:,1],'bo')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS-lA46fyHaN"
   },
   "source": [
    "You should expect the mini-batch gradient descent produces results which are somewhat intermediate between the stochastic and batch versions.  \n",
    "\n",
    "It's worth noting that for this example, stochastic gradient descent takes quite a bit more time to run.  This is because our dataset is relatively small, and the problem we are trying to solve is relatively simple.  However, in large scale problems (think running neural networks over millions of images), it's not possible to fit the training set into memory, and the computation becomes overwhelming.  Simultaneously, in cases where there are many local minimina, SGD may perform better because some local minima may only form for a large number of data points simultaneously.  In this sense, it may also be viewed as a form of *regularization*, because it helps the model avoid overfitting.\n",
    "\n",
    "## Exercise: Implement optimization in various forms\n",
    "\n",
    "### Momentum\n",
    "\n",
    "One popular variant on gradient descent is the inclusion of momentum.  Remember that momentum applies the concept of exponential decay to keep a running average of past gradients. Momentum utilizes this information for the following parameter update:\n",
    "$$\n",
    "\\Delta \\mathbf{w}_i = m \\Delta \\mathbf{w}_{i-1} + (1-m) \\nabla \\mathbf{w}_i\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_i - \\eta \\Delta \\mathbf{w}_i\n",
    "$$\n",
    "This effectively makes the update direction slower to change, and can help to push the model up and out of local minima.  Let's illustrate it's function using stochastic gradient descent (mini-batch size 1). Your job is to implement this by adjusting the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiR4AL6DyHaN"
   },
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize weight storage\n",
    "w_momen = [w.copy()]\n",
    "w_mepoch = [w.copy()]\n",
    "\n",
    "# Define learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Define momentum parameter\n",
    "momentum = 0.9\n",
    "\n",
    "# Define a variable to hold the update step from the previous time step\n",
    "delta_w = 0.0\n",
    "\n",
    "# Loop over 10000 epochs\n",
    "for i in range(10000):\n",
    "\n",
    "    # Shuffle the data points\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    \n",
    "    # Loop over all the shuffled data points\n",
    "    for j in random_indices:\n",
    "        \n",
    "        # Draw the relevant data point from the big data array\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "\n",
    "        # Compute the gradient\n",
    "        gradient = G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        \n",
    "        # Compute the update direction (no longer just the gradient: now a weighted average of the\n",
    "        # gradient with previous gradients)\n",
    "        \n",
    "        # TODO: Implement the calculation of delta_w below\n",
    "        delta_w = None \n",
    "        \n",
    "        # Update weights\n",
    "        w -= eta*delta_w\n",
    "        \n",
    "        # Store weights\n",
    "        w_momen.append(w.copy())\n",
    "    \n",
    "    # Store epoch weights\n",
    "    w_mepoch.append(w.copy())\n",
    "\n",
    "# convert lists to arrays\n",
    "w_momen = np.array(w_momen)\n",
    "w_mepoch = np.array(w_mepoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfoHtnbxyHaO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot error surface\n",
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "\n",
    "#Plot batch gradient descent, SGD, and SGD w/ momentum\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpvVgTVeyHaO"
   },
   "outputs": [],
   "source": [
    "# Same as above but zoomed in\n",
    "\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XhMmvdYyHaO"
   },
   "source": [
    "Momentum reduces the size of the wiggles due to the stochasticity in stochastic gradient descent.  \n",
    "\n",
    "### RMSprop\n",
    "A popular variant of gradient descent is called RMSprop, and it is similar to gradient descent with momentum, but with a twist: instead of keeping a running average of the gradient, RMSprop keeps a running average of the squared gradient.  Then, when it comes time to update the weights, it normalizes the gradient by the square-root of this average-squared gradient. \n",
    "$$\n",
    "\\bar{g^2}_i = m \\bar{g^2}_{i-1} + (1-m) (\\nabla \\mathbf{w}_i)^2\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_i - \\eta \\frac{\\nabla \\mathbf{w}_i}{\\sqrt{\\bar{g^2}}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "What does this do?  It effectively eliminates the scale of the gradient from the problem, and we only go downhill based on the sign, which is very useful when the gradients of the different parameters are very different from one another.  The momentum is necessary because the sign of the gradient can jump around alot with SGD, so it's better to know *generally* which direction is down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCu1lx2ZyHaP"
   },
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize storage\n",
    "w_momen = [w.copy()]\n",
    "w_mepoch = [w.copy()]\n",
    "\n",
    "# Set learning rate (Lower because this method learns really fast!)\n",
    "eta = 0.0001\n",
    "\n",
    "# Set momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Initialize average squared gradient\n",
    "g2 = 0\n",
    "\n",
    "# Iterate over 10000 epochs\n",
    "for i in range(10000):\n",
    "\n",
    "    # Randomly shuffle indices\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    \n",
    "    # Loop over shuffled indices\n",
    "    for j in random_indices:\n",
    "        \n",
    "        # Extract single data point\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        \n",
    "        # Compute the gradient\n",
    "        gradient = G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        \n",
    "        # TODO: Update averaged square gradient\n",
    "        g2 = None\n",
    "        \n",
    "        #TODO: Update with gradient normalized by root square average gradient\n",
    "        w -= None\n",
    "        \n",
    "        # Store weight values\n",
    "        w_momen.append(w.copy())\n",
    "    w_mepoch.append(w.copy())       \n",
    "w_momen = np.array(w_momen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxFL3VZuyHaP"
   },
   "outputs": [],
   "source": [
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[::1000,0],w_momen[::1000,1],'ro-')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ndQU03ryHaP"
   },
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.9)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfJqHwmiyHaQ"
   },
   "source": [
    "In practice, RMSprop can be combined with normal momentum.  \n",
    "\n",
    "These are just a few examples of the large scale gradient descent schemes that can be used for general optimization problems, but especially neural networks.  There are many, many other methods (a good overview can be found [here](http://ruder.io/optimizing-gradient-descent/) ).  However, effectively, these are all just slight variations on the general theme of figuring out which way is down, and going that direction.\n",
    "\n",
    "Momentum reduces the size of the wiggles due to the stochasticity in stochastic gradient descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4FhzOgzyHaQ"
   },
   "source": [
    "\n",
    "### AdaGrad\n",
    "\n",
    "Another optimizer that is similar to RMSProp is AdaGrad. This method adapts the learning rate to the parameters. Effectively, it performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features.\n",
    "\n",
    "This optimizer keeps track of the second moment of the gradient (think square) by summing it over all iterations. The result is that AdaGrad does well with cases where there are sparse gradients, for example in sparse datasets or near the end of optimization. \n",
    "\n",
    "It performs the update as follows:\n",
    "$$\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_i - \\eta \\frac{\\nabla \\mathbf{w}_i}{\\sqrt{\\bar{G_{ii}+ \\epsilon}}}\n",
    "$$\n",
    "where $\\mathbf{G_{ii}}$ is the sum of the current and all past diagonals of the matrix resulting from the multiplication of the gradient vector by itself. This can be optimized by simply performing the dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coChrmamyHaQ"
   },
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize storage\n",
    "w_momen = [w.copy()]\n",
    "w_mepoch = [w.copy()]\n",
    "\n",
    "# Set learning rate (Lower because this method learns really fast!)\n",
    "eta = 0.01\n",
    "\n",
    "# Set momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Initialize sum of squared gradients\n",
    "g2 = 0\n",
    "\n",
    "# Iterate over 10000 epochs\n",
    "for i in range(10000):\n",
    "\n",
    "    # Randomly shuffle indices\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    \n",
    "    # Loop over shuffled indices\n",
    "    for j in random_indices:\n",
    "        \n",
    "        # Extract single data point\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        \n",
    "        # Compute the gradient\n",
    "        gradient = G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        \n",
    "        #TODO: Update sum of all squared gradients\n",
    "        g2 = None\n",
    "        \n",
    "        #TODO: Update with gradient normalized by root square summed gradient\n",
    "        w -= None\n",
    "        \n",
    "        # Store weight values\n",
    "        w_momen.append(w.copy())\n",
    "    w_mepoch.append(w.copy())       \n",
    "w_momen = np.array(w_momen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3yLxTcxyHaQ"
   },
   "outputs": [],
   "source": [
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[::1000,0],w_momen[::1000,1],'ro-')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQWPD7H8yHaQ"
   },
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.9)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRJZes08yHaR"
   },
   "source": [
    "\n",
    "### Adam\n",
    "\n",
    "Adam is an optimizer that performs well in a variety of settings due to the various improvements it makes over vanilla gradient descent. \n",
    "\n",
    "* Firstly, it implements momentum by keeping track of the gradients calculated at each step. By doing this and using it in the update step, it also does well in settings where gradients are sparse. \n",
    "* Secondly, it corrects for optimization bias that occurrs when using momentum. Few other methods do this. By doing so it improves the rate of optimization in early stages and can move toward convergence sooner than other methods. \n",
    "* Thirdly, and perhaps the most important differentiator of this method, it anneals the size of the update using a ratio of a running average of the first moment of the gradient with the square root of a running average of the second moment of the gradient. This means that the update step is invariant to the scale of the gradient and performs its update with respect to 1) the direction of the slope in the loss landscape and 2) the size of alpha.\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m_t} = m_t / (1-\\beta_1^t)\n",
    "$$\n",
    "$$\n",
    "\\hat{v_t} = v_t / (1-\\beta_2^t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_i - \\eta \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}}+ \\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itll1qn-yHaR"
   },
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w = np.array([0.,0.])\n",
    "\n",
    "# Initialize storage\n",
    "w_momen = [w.copy()]\n",
    "w_mepoch = [w.copy()]\n",
    "\n",
    "# Set learning rate (Lower because this method learns really fast!)\n",
    "eta = 0.0001\n",
    "\n",
    "# Set parameters beta_1 and beta_2\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "\n",
    "# Initialize exponentially decaying average of first moment\n",
    "m = 0\n",
    "# Initialize exponentially decaying average of second moment\n",
    "v = 0\n",
    "\n",
    "# Iterate over 10000 epochs\n",
    "for i in range(1,10001):\n",
    "\n",
    "    # Randomly shuffle indices\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    \n",
    "    # Loop over shuffled indices\n",
    "    for j in random_indices:\n",
    "        \n",
    "        # Extract single data point\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        \n",
    "        # Compute the gradient\n",
    "        gradient = G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        \n",
    "        #TODO: Update moments\n",
    "        m = None \n",
    "        v = None \n",
    "        \n",
    "        #TODO: Correct moment bias\n",
    "        m_hat = None\n",
    "        v_hat = None\n",
    "\n",
    "        #TODO: Update with gradient normalized by root square summed gradient\n",
    "        w -= None\n",
    "        \n",
    "        # Store weight values\n",
    "        w_momen.append(w.copy())\n",
    "    w_mepoch.append(w.copy())       \n",
    "w_momen = np.array(w_momen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-bJCI-eyHaR"
   },
   "outputs": [],
   "source": [
    "plt.contour(w0s,w1s,L_grid,20)\n",
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[::1000,0],w_momen[::1000,1],'ro-')\n",
    "plt.axis('equal')\n",
    "#plt.xlim(1.1,1.2)\n",
    "#plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UrSpkqUyHaR"
   },
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.9)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercise: Adjusting hyperparameters\n",
    "\n",
    "The hyperparameters associated with these algorithms can make a singificant difference to how they find optimal parameters. Try experimenting with the optimizers you implemented above and think about the following:\n",
    "* Run the optimizers again using eta values that range from 1e-2 to 1e-4. What are suitable ranges of learning rate for the different optimizers? Why do they differ?\n",
    "* Try different values for the momentum parameter (remember that in the case of Adam there are two). What impact does this have on the plots generated and why?\n",
    "* Note the different step sizes in the plots generated for the different optimizers; think about what you observe and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-ArfAq0yHaR"
   },
   "source": [
    "## Optional Extra Learning Exercise:\n",
    "Pytorch has all of these methods built in (and many more). You can explore some of them with the MNIST dataset, using the following code as a starting point. Specifically, try the following two experiments.\n",
    "\n",
    "First, train for 50 (or more) epochs using three different optimization methods (options include SGD, Adagrad, RMSprop, and Adam, but you are free to use whatever you like; Documentation and options are [here](https://pytorch.org/docs/stable/optim.html)).  Record the loss and/or accuracy at each epoch for each method, and plot them all together. Identify which optimization method works best for this problem and discuss why you think this is the case.\n",
    "\n",
    "Second, using the best method that you identified above, experiment with different hyperparameters as they are mentioned in the documentation. Note performance differences (in the sense of computational speed) between the different hyperparameter settings and discuss why this might be the case.\n",
    "\n",
    "Model specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKQgt-mCyHaS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(784,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,10)\n",
    "        \n",
    "   \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        # Apply dropout to the input\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        \n",
    "        # Apply dropout to the hidden layer\n",
    "        a2 = self.l2(z1)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoYokM7JyHaS"
   },
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wt0tdyaeyHaS"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True, as_frame=False)\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y,test_size=10000)\n",
    "\n",
    "# Extract number of data points, and the height and width of the images for later reshaping\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "h = 28\n",
    "w = 28\n",
    "\n",
    "N = 10\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WklzelLZyHaS"
   },
   "source": [
    "Example Training Loop for SGD (boilerplate code for implementing different optimizers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLaIoXgfyHaT"
   },
   "outputs": [],
   "source": [
    "SGD_loss = []\n",
    "SGD_accuracy = []\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "### Stochastic Gradient Descent ###\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3)\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)\n",
    "    SGD_loss.append(loss.item())\n",
    "    SGD_accuracy.append(100.*correct/total)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "optimizers.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
